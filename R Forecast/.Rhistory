distinct(player_id, tID) %>%
mutate(across(c(player_id, tID), as.integer))
if (nrow(active_players_pos) == 0) {
cat("   No active players found for", toupper(pos), "in Historical GW", last_historical_gw, ". Skipping.\n")
next
}
base_input_data_pos <- scaled_data %>%
filter(player_id %in% active_players_pos$player_id) %>%
group_by(player_id) %>%
arrange(GW) %>%
slice_tail(n = vindu) %>%
filter(n() == vindu) %>%
ungroup()
players_with_history <- base_input_data_pos %>% distinct(player_id)
active_players_pos <- active_players_pos %>% filter(player_id %in% players_with_history$player_id)
n_forecast_pos <- nrow(active_players_pos)
if (n_forecast_pos == 0) {
cat("   No players with sufficient history (\", vindu, \"GWs) found for\", toupper(pos), \". Skipping.\n")
next
}
cat("   Base input data prepared for", n_forecast_pos, "players based on data up to Historical GW", last_historical_gw, ".\n")
num_list <- base_input_data_pos %>%
select(player_id, all_of(current_numF)) %>%
arrange(match(player_id, active_players_pos$player_id)) %>%
group_by(player_id) %>%
group_split(.keep = FALSE) %>%
lapply(as.matrix)
base_num_array_pos <- array(
unlist(num_list),
dim = c(n_forecast_pos, vindu, length(current_numF))
)
base_cat_player_id_pos <- matrix(as.integer(active_players_pos$player_id), ncol = 1)
base_cat_tID_pos <- matrix(as.integer(active_players_pos$tID), ncol = 1)
base_input_list[[pos]] <- list(
active_players = active_players_pos,
num_array = base_num_array_pos,
cat_player_id = base_cat_player_id_pos,
cat_tID = base_cat_tID_pos,
mu = current_mu,
sigma = current_sigma
)
}
all_player_metadata_df <- bind_rows(all_player_metadata) %>% distinct(player_id, .keep_all = TRUE)
# 4. Forecast Loop (Predicting for each fixture in target GWs)
#--------------------------------------------
all_fixture_forecasts_list <- list() # Store predictions PER FIXTURE
for (gw_api_to_predict in target_api_gws) {
cat("\n--- Forecasting for API GW:", gw_api_to_predict, "---\n")
future_fixtures_gw <- future_fixtures_filtered %>% filter(GW == gw_api_to_predict)
if (nrow(future_fixtures_gw) == 0) {
cat("   WARNING: No fixture data found for API GW", gw_api_to_predict, ". Skipping this GW.\n")
next
}
forecasts_this_gw_list <- list()
for (pos in names(base_input_list)) {
cat("      Forecasting position:", toupper(pos), "\n")
base_inputs <- base_input_list[[pos]]
current_model <- model_list[[pos]]
# Prepare inputs for *each fixture* this GW for active players in this position
fixture_inputs_gw_pos <- base_inputs$active_players %>%
inner_join(future_fixtures_gw, by = c("tID" = "team_id"), relationship = "many-to-many") %>% # Join players to ALL their fixtures this GW
select(player_id, tID, opponent_id = opponent_id, is_home = is_home) %>%
arrange(player_id) # Arrange by player_id to group fixtures
if (nrow(fixture_inputs_gw_pos) == 0) {
cat("         No players with fixtures found for this position in GW", gw_api_to_predict, "\n")
next
}
# Identify which players have DGWs *within this position's active players*
player_fixture_counts <- fixture_inputs_gw_pos %>% count(player_id)
dgw_players_pos <- player_fixture_counts %>% filter(n > 1) %>% pull(player_id)
sgw_players_pos <- player_fixture_counts %>% filter(n == 1) %>% pull(player_id)
cat("         Found", length(dgw_players_pos), "DGW players and", length(sgw_players_pos), "SGW players.\n")
# Prepare arrays - need to potentially duplicate base arrays for DGW players
# Find indices of players in the original base arrays
player_indices <- match(fixture_inputs_gw_pos$player_id, base_inputs$active_players$player_id)
# Replicate base arrays according to the fixture list
num_array_predict <- base_inputs$num_array[player_indices, , , drop = FALSE]
cat_player_id_predict <- base_inputs$cat_player_id[player_indices, , drop = FALSE]
cat_tID_predict <- base_inputs$cat_tID[player_indices, , drop = FALSE]
# Prepare dynamic categorical inputs (already ordered by player_id)
cat_oID_predict <- matrix(as.integer(fixture_inputs_gw_pos$opponent_id), ncol = 1)
cat_hID_predict <- matrix(as.integer(fixture_inputs_gw_pos$is_home), ncol = 1)
# Dimension check
stopifnot(
nrow(num_array_predict) == nrow(cat_player_id_predict),
nrow(num_array_predict) == nrow(cat_tID_predict),
nrow(num_array_predict) == nrow(cat_oID_predict),
nrow(num_array_predict) == nrow(cat_hID_predict),
nrow(num_array_predict) == nrow(fixture_inputs_gw_pos) # Ensure all dimensions match the number of fixtures to predict
)
# Predict for ALL fixtures at once
cat("         Running prediction for", nrow(num_array_predict) ,"fixtures...\n")
pred_future_scaled <- current_model %>% predict(
list(
input_seq = num_array_predict,
input_player_id = cat_player_id_predict,
input_tID = cat_tID_predict,
input_oID = cat_oID_predict,
input_hID = cat_hID_predict
)
)
# Store raw scaled predictions PER FIXTURE
future_preds_per_fixture <- tibble(
player_id = fixture_inputs_gw_pos$player_id,
GW = gw_api_to_predict, # API GW number
position = pos,
opponent_id = fixture_inputs_gw_pos$opponent_id,
is_home = fixture_inputs_gw_pos$is_home,
predicted_points_scaled_fixture = as.vector(pred_future_scaled)
)
forecasts_this_gw_list[[pos]] <- future_preds_per_fixture
cat("         Raw fixture forecasts stored for", nrow(future_preds_per_fixture), "fixtures.\n")
}
# Add all fixture forecasts for this GW to the main list
all_fixture_forecasts_list[[as.character(gw_api_to_predict)]] <- bind_rows(forecasts_this_gw_list)
}
# 5. Aggregate Predictions per GW, Unscale, and Finalize
#--------------------------------------------
future_forecast_df_raw_fixtures <- bind_rows(all_fixture_forecasts_list)
if (nrow(future_forecast_df_raw_fixtures) == 0) {
stop("No future fixture forecasts were generated.")
}
cat("\n--- Aggregating, Unscaling and Finalizing Forecasts ---\n")
# Aggregate scaled predictions by summing per player per GW
future_forecast_df_aggregated_scaled <- future_forecast_df_raw_fixtures %>%
group_by(player_id, GW, position) %>%
summarise(
predicted_total_points_scaled = sum(predicted_points_scaled_fixture, na.rm = TRUE),
num_fixtures = n(), # Keep track of how many fixtures were summed
# Keep first opponent/home status for display purposes (or create a combined string)
first_opponent_id = first(opponent_id),
first_is_home = first(is_home),
.groups = 'drop'
)
# Unscale the *aggregated* predictions
future_forecast_df_unscaled <- future_forecast_df_aggregated_scaled %>%
mutate(
mu = map_dbl(position, ~ scaling_factors[[.x]]$mu),
sigma = map_dbl(position, ~ scaling_factors[[.x]]$sigma),
predicted_total_points = predicted_total_points_scaled * sigma + mu,
predicted_total_points = round(predicted_total_points, 2)
) %>%
select(-predicted_total_points_scaled, -mu, -sigma)
# 6. Translate IDs to Strings
#--------------------------------------------
if (!exists("team_mapping") || !all(c("team_id", "team_name") %in% names(team_mapping))) {
stop("The 'team_mapping' dataframe is missing or does not have 'team_id' and 'team_name' columns.")
}
team_mapping <- team_mapping %>% mutate(team_id = as.integer(team_id))
future_forecast_df_final <- future_forecast_df_unscaled %>%
left_join(all_player_metadata_df, by = "player_id", suffix = c("", ".meta")) %>%
# Join opponent team name based on the *first* opponent ID
left_join(team_mapping %>% select(first_opponent_id = team_id, opponent_team_name = team_name),
by = "first_opponent_id") %>%
# Handle NO_FIXTURE case (if opponent ID was 0)
mutate(opponent_display = ifelse(first_opponent_id == 0, "NO_FIXTURE",
ifelse(num_fixtures > 1, paste0(opponent_team_name, " (DGW)"), opponent_team_name)),
is_home_display = ifelse(first_opponent_id == 0, NA_character_,
ifelse(num_fixtures > 1, paste0(first_is_home, " (DGW)"), as.character(first_is_home)))
) %>%
# Select and arrange final columns
select(
GW, # API GW
player_id,
name,
position = position,
team = team,
value = value,
opponent_display, # Shows first opponent, notes DGW
is_home_display,  # Shows first fixture home/away, notes DGW
num_fixtures,     # Number of fixtures in this GW
predicted_points = predicted_total_points
) %>%
arrange(GW, desc(predicted_points))
cat("Final forecast data frame created with team names and DGW handling.\n")
glimpse(future_forecast_df_final)
# 7. Export the Final Forecasts
#--------------------------------------------
future_filename <- paste0("Future_Forecasts_API_GW", min(target_api_gws), "_to_GW", max(target_api_gws), "_Final_DGW_Summed.csv")
write_csv(future_forecast_df_final, future_filename)
cat("Final future forecasts saved to", future_filename, "\n")
# Forecasts for FPL Script to Master thesis EBA&PAM
# To load save weights ----
# Kaggle Export: The .hdf5 weight files saved in /kaggle/working/ will appear in the "Output" section of your Kaggle notebook session after it completes. You can download them from there.
# Loading Later: Remember, when you want to use these weights later (in another notebook or locally), you MUST:
# Rebuild the exact same model architecture using keras_model().
# Compile the model using the exact same optimizer, loss, and metrics.
# Load the weights using load_model_weights_hdf5().
# Load the corresponding scaling factors (mu, sigma, numF) to prepare input data and unscale predictions.
rm(list = ls(all = TRUE))
## 0: Preamble ----
Sys.setenv(WORKON_HOME = "C:/Users/peram/Documents/test/R Forecast/.virtualenvs")
reticulate::use_virtualenv("C:/Users/peram/Documents/test/R Forecast/.virtualenvs/r-reticulate", required = TRUE)
# Adjust code to chosen place of
# 1. Virtual Python Environment
# 2. Forces R session to use the a specific virtualenv
# This way the virtual environment does not get installed
# in your onedrive\documents folder.
#install.packages("keras")
library(reticulate)
#virtualenv_create("r-reticulate", python = install_python())
library(keras)
#install_keras(envname = "r-reticulate")
# Rest of the packages
if (!require("pacman")) install.packages("pacman")
pacman::p_load(ggthemes, tidyverse, slider,
slider,glmnet,httr,jsonlite,tensorflow,
randomForest, Metrics, pastecs,
stats)
# Check if Keras and TF is properly installed
tf$constant("Hello, TensorFlow!")
# Lists to store models and scaling factors per position
model_list <- list()
scaling_factors <- list()
# Global user inputs ----
# Trainingsplit : in-sample <= split_gw < out-of-sample
split_gw <- 38+38
# LSTM
epoker <- 15
vindu <- 3
#Metrics for LSTM Models. Alle fungerer ikke fordi noen er for keras 3 eller heter noe annet som jeg ikke finner
metrics_regression <- c(
# metric_mean_absolute_error(),
# metric_mean_squared_error(),
metric_root_mean_squared_error()
# metric_mean_absolute_percentage_error(),
# metric_cosine_similarity()
)
# Til Prognosering, hvor mange uker frem ønsker du å spå fremover
antall_uker <- 7
# Til patience epoker i callback.
# Altså hvor mange epoker skal den vente før den slutter å trene modellen og gå tilbake til siste beste
num_patience <- 5
# Decided embedding dimensions for pos.6 Building the Model
num_embedding_dimension <- 1
# Fetch data and prelim data manipulation ----
df <- read_csv("C:/Users/peram/Documents/test/Datasett/Ekstra kolonner, stigende GW, alle tre sesonger(22-24), heltall.csv")
alternativsammensatt <- df
## Position partitions ----
gk <- alternativsammensatt |>
filter(position == "GK")
def <- alternativsammensatt |>
filter(position == "DEF")
mid <- alternativsammensatt |>
filter(position == "MID")
fwd <- alternativsammensatt |>
filter(position == "FWD")
posliste <- list(gk = gk, def = def, mid = mid, fwd = fwd)
lapply(posliste, head, n = 1);lapply(posliste, tail, n = 1)
str(gk)
unscaled_gk <- gk
unscaled_def <- def
unscaled_mid <- mid
unscaled_fwd <- fwd
# EDA ----
# Where to save plots?
plot_directory <- "C:/Users/peram/Documents/test/R Forecast/EDA"
dir.create(plot_directory, recursive = TRUE)
# Define position order for consistent plotting
position_levels <- c("GK", "DEF", "MID", "FWD")
# Split data by season (assuming alternativsammensatt contains multiple seasons)
# Filter for season boundaries based on GW
season_22_23 <- alternativsammensatt |> filter(GW <= 38)
season_23_24 <- alternativsammensatt |> filter(GW > 38 , GW < 38+38+1)
# Create datasets of starting players
starters_22_23 <- season_22_23 |>
filter(starts == 1) |>
select(total_points, position) |>
mutate(position = factor(position, levels = position_levels))
starters_23_24 <- season_23_24 |>
filter(starts == 1) |>
select(total_points, position) |>
mutate(position = factor(position, levels = position_levels))
# Display dataset structure
glimpse(starters_22_23)
cat("Position levels consistent between seasons:",
identical(levels(starters_22_23$position), levels(starters_23_24$position)), "\n")
# Create boxplots for point distributions by position for starters
plot1 <- ggplot(starters_22_23, aes(x = position, y = total_points)) +
geom_boxplot() +
labs(title = "Point distribution for starting players in season 22/23",
x = "Position",
y = "Observed points per week") +
scale_y_continuous(breaks = seq(0, max(starters_22_23$total_points, na.rm = TRUE), 2))
plot2 <- ggplot(starters_23_24, aes(x = position, y = total_points)) +
geom_boxplot() +
labs(title = "Point distribution for starting players in season 23/24",
x = "Position",
y = "Observed points per week") +
scale_y_continuous(breaks = seq(0, max(starters_23_24$total_points, na.rm = TRUE), 2))
# Display plots
plot1
plot2
# Create datasets of players with at least 15 minutes
players_15min_22_23 <- season_22_23 |>
filter(minutes >= 15) |>
select(total_points, position) |>
mutate(position = factor(position, levels = position_levels))
players_15min_23_24 <- season_23_24 |>
filter(minutes >= 15) |>
select(total_points, position) |>
mutate(position = factor(position, levels = position_levels))
# Create boxplots for players with at least 15 minutes
plot3 <- ggplot(players_15min_22_23, aes(x = position, y = total_points)) +
geom_boxplot() +
labs(title = "Point distribution for players with ≥15 minutes (22/23)",
x = "Position",
y = "Observed points per week") +
scale_y_continuous(breaks = seq(0, max(players_15min_22_23$total_points, na.rm = TRUE), 2))
plot4 <- ggplot(players_15min_23_24, aes(x = position, y = total_points)) +
geom_boxplot() +
labs(title = "Point distribution for players with ≥15 minutes (23/24)",
x = "Position",
y = "Observed points per week") +
scale_y_continuous(breaks = seq(0, max(players_15min_23_24$total_points, na.rm = TRUE), 2))
# Display plots
plot3
plot4
# Save plots starting players
ggsave(file.path(plot_directory, "Point distribution for starting players in season 22-23.png"),
plot = plot1, width = 8, height = 6)
ggsave(file.path(plot_directory, "Point distribution for starting players in season 23-24.png"),
plot = plot2, width = 8, height = 6)
# Save plots 15 min play time or more
ggsave(file.path(plot_directory, "Point distribution for players with 15min+ playing time 22-23.png"),
plot = plot3, width = 8, height = 6)
ggsave(file.path(plot_directory, "Point distribution for players with 15min+ playing time 23-24.png"),
plot = plot4, width = 8, height = 6)# Hvordan er fordelingen av poeng for spillere som starter?
## Statistical summaries ----
# Create statistical summary tables
stats_starters_22_23 <- stat.desc(starters_22_23$total_points)
stats_starters_23_24 <- stat.desc(starters_23_24$total_points)
stats_15min_22_23 <- stat.desc(players_15min_22_23$total_points)
stats_15min_23_24 <- stat.desc(players_15min_23_24$total_points)
# Combine statistics for comparison
stats_comparison <- data.frame(
Metric = rownames(as.data.frame(stats_starters_22_23)),
Starters_22_23 = stats_starters_22_23,
Starters_23_24 = stats_starters_23_24,
Min15_22_23 = stats_15min_22_23,
Min15_23_24 = stats_15min_23_24
)
print(stats_comparison)
## Distribution density plots for starting players ----
# Create distribution plots with different colors by position
dist_plot1 <- ggplot(starters_22_23, aes(x = total_points, fill = position, color = position)) +
geom_density(alpha = 0.2) +
scale_color_brewer(palette = "Set1") +
scale_fill_brewer(palette = "Set1") +
labs(title = "Point distribution for starting players in season 22/23",
x = "Total points",
y = "Density") +
theme_minimal() +
scale_x_continuous(breaks = seq(0, max(starters_22_23$total_points, na.rm = TRUE), 2))
dist_plot2 <- ggplot(starters_23_24, aes(x = total_points, fill = position, color = position)) +
geom_density(alpha = 0.2) +
scale_color_brewer(palette = "Set1") +
scale_fill_brewer(palette = "Set1") +
labs(title = "Point distribution for starting players in season 23/24",
x = "Total points",
y = "Density") +
theme_minimal() +
scale_x_continuous(breaks = seq(0, max(starters_23_24$total_points, na.rm = TRUE), 2))
# Display density plots for starters
dist_plot1
dist_plot2
# Save density plots for starters
ggsave(file.path(plot_directory, "Point density distribution for starting players 22-23.png"),
plot = dist_plot1, width = 10, height = 6)
ggsave(file.path(plot_directory, "Point density distribution for starting players 23-24.png"),
plot = dist_plot2, width = 10, height = 6)
# Distribution plots for players with at least 15 minutes
dist_plot3 <- ggplot(players_15min_22_23, aes(x = total_points, fill = position, color = position)) +
geom_density(alpha = 0.2) +
scale_color_brewer(palette = "Set1") +
scale_fill_brewer(palette = "Set1") +
labs(title = "Point distribution for players with ≥15 minutes (22/23)",
x = "Total points",
y = "Density") +
theme_minimal() +
scale_x_continuous(breaks = seq(0, max(players_15min_22_23$total_points, na.rm = TRUE), 2))
dist_plot4 <- ggplot(players_15min_23_24, aes(x = total_points, fill = position, color = position)) +
geom_density(alpha = 0.2) +
scale_color_brewer(palette = "Set1") +
scale_fill_brewer(palette = "Set1") +
labs(title = "Point distribution for players with ≥15 minutes (23/24)",
x = "Total points",
y = "Density") +
theme_minimal() +
scale_x_continuous(breaks = seq(0, max(players_15min_23_24$total_points, na.rm = TRUE), 2))
# Display density plots for 15+ minute players
dist_plot3
dist_plot4
# Save density plots for 15+ minute players
ggsave(file.path(plot_directory, "Point density distribution for players with 15min+ 22-23.png"),
plot = dist_plot3, width = 10, height = 6)
ggsave(file.path(plot_directory, "Point density distribution for players with 15min+ 23-24.png"),
plot = dist_plot4, width = 10, height = 6)
# Alternative overlaid distributions (combined in single plots)
# This shows the direct comparison between seasons for each position
# For starters: Compare 22/23 vs 23/24 by position
combined_starters <- bind_rows(
mutate(starters_22_23, season = "22/23"),
mutate(starters_23_24, season = "23/24")
)
pos_dist_starters <- ggplot(combined_starters, aes(x = total_points, color = season)) +
geom_density(linewidth = 1) +
facet_wrap(~ position, scales = "free_y") +
scale_color_manual(values = c("22/23" = "#1b9e77", "23/24" = "#d95f02")) +
labs(title = "Comparing point distributions across seasons for starting players",
x = "Total points",
y = "Density") +
theme_minimal()
pos_dist_starters
# For 15+ minute players: Compare 22/23 vs 23/24 by position
combined_15min <- bind_rows(
mutate(players_15min_22_23, season = "22/23"),
mutate(players_15min_23_24, season = "23/24")
)
pos_dist_15min <- ggplot(combined_15min, aes(x = total_points, color = season)) +
geom_density(linewidth = 1) +
facet_wrap(~ position, scales = "free_y") +
scale_color_manual(values = c("22/23" = "#1b9e77", "23/24" = "#d95f02")) +
labs(title = "Comparing point distributions across seasons for players with ≥15 minutes",
x = "Total points",
y = "Density") +
theme_minimal()
pos_dist_15min
# Save the combined comparison plots
ggsave(file.path(plot_directory, "Season comparison starters by position.png"),
plot = pos_dist_starters, width = 10, height = 8)
ggsave(file.path(plot_directory, "Season comparison 15min+ by position.png"),
plot = pos_dist_15min, width = 10, height = 8)
## QQ Plots to check normality ----
# QQ Plot for starters 22/23
png(file.path(plot_directory, "QQ Plot of Starters 22-23 Points.png"), width = 800, height = 600)
qqnorm(starters_22_23$total_points, col = "blue", pch = 20,
main = "Q-Q Plot of Starters 22/23 Points",
xlab = "Theoretical Quantiles",
ylab = "Sample Quantiles")
qqline(starters_22_23$total_points, col = "red", lwd = 2)
dev.off()
# QQ Plot for starters 23/24
png(file.path(plot_directory, "QQ Plot of Starters 23-24 Points.png"), width = 800, height = 600)
qqnorm(starters_23_24$total_points, col = "blue", pch = 20,
main = "Q-Q Plot of Starters 23/24 Points",
xlab = "Theoretical Quantiles",
ylab = "Sample Quantiles")
qqline(starters_23_24$total_points, col = "red", lwd = 2)
dev.off()
# QQ Plot for 15+ minutes players 22/23
png(file.path(plot_directory, "QQ Plot of Players 15min+ 22-23 Points.png"), width = 800, height = 600)
qqnorm(players_15min_22_23$total_points, col = "blue", pch = 20,
main = "Q-Q Plot of Players with ≥15 Minutes 22/23 Points",
xlab = "Theoretical Quantiles",
ylab = "Sample Quantiles")
qqline(players_15min_22_23$total_points, col = "red", lwd = 2)
dev.off()
# QQ Plot for 15+ minutes players 23/24
png(file.path(plot_directory, "QQ Plot of Players 15min+ 23-24 Points.png"), width = 800, height = 600)
qqnorm(players_15min_23_24$total_points, col = "blue", pch = 20,
main = "Q-Q Plot of Players with ≥15 Minutes 23/24 Points",
xlab = "Theoretical Quantiles",
ylab = "Sample Quantiles")
qqline(players_15min_23_24$total_points, col = "red", lwd = 2)
dev.off()
## Additional analysis - Points by position ----
# Create summary statistics by position for each dataset
position_stats_starters_22_23 <- starters_22_23 |>
group_by(position) |>
summarize(
Count = n(),
Mean = mean(total_points, na.rm = TRUE),
Median = median(total_points, na.rm = TRUE),
SD = sd(total_points, na.rm = TRUE),
Min = min(total_points, na.rm = TRUE),
Max = max(total_points, na.rm = TRUE)
)
position_stats_starters_23_24 <- starters_23_24 |>
group_by(position) |>
summarize(
Count = n(),
Mean = mean(total_points, na.rm = TRUE),
Median = median(total_points, na.rm = TRUE),
SD = sd(total_points, na.rm = TRUE),
Min = min(total_points, na.rm = TRUE),
Max = max(total_points, na.rm = TRUE)
)
# Print position-based statistics
cat("Position statistics for starters 22/23:\n")
print(position_stats_starters_22_23)
cat("\nPosition statistics for starters 23/24:\n")
print(position_stats_starters_23_24)
# Forecasts for FPL Script to Master thesis EBA&PAM
# To load save weights ----
# Kaggle Export: The .hdf5 weight files saved in /kaggle/working/ will appear in the "Output" section of your Kaggle notebook session after it completes. You can download them from there.
# Loading Later: Remember, when you want to use these weights later (in another notebook or locally), you MUST:
# Rebuild the exact same model architecture using keras_model().
# Compile the model using the exact same optimizer, loss, and metrics.
# Load the weights using load_model_weights_hdf5().
# Load the corresponding scaling factors (mu, sigma, numF) to prepare input data and unscale predictions.
rm(list = ls(all = TRUE))
## 0: Preamble ----
Sys.setenv(WORKON_HOME = "C:/Users/peram/Documents/test/R Forecast/.virtualenvs")
reticulate::use_virtualenv("C:/Users/peram/Documents/test/R Forecast/.virtualenvs/r-reticulate", required = TRUE)
library(reticulate)
library(keras)
if (!require("pacman")) install.packages("pacman")
pacman::p_load(ggthemes, tidyverse, slider,
glmnet,httr,jsonlite,tensorflow,
Metrics, pastecs, stats, png, grid)
model%>%compile(optimizer=optimizer_adam(),loss="mse",metrics=metric_root_mean_squared_error())
